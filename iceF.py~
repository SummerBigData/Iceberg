
import numpy as np

# Keras stuff
import keras
from keras import regularizers
from keras.models import Sequential, load_model, Model
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Activation, Input
from keras.layers import Conv2DTranspose, Reshape, UpSampling2D, concatenate
from keras.optimizers import Adam
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau
from keras.utils import plot_model
import os.path # To check if a file exists
import iceDataPrep


#---------GLOBAL VARIABLES----------GLOBAL VARIABLES----------GLOBAL VARIABLES----------GLOBAL VARIABLES


# fix random seed for reproducibility
np.random.seed(7)




#----------DEFINITIONS HERE----------DEFINITIONS HERE----------DEFINITIONS HERE----------DEFINITIONS HERE



def ShowSquare(band1, band2): 
	hspace = np.zeros((75, 5, 3))
	vspace = np.zeros((5, 5*75 + 5*6, 3))
	picAll = vspace
	for i in range(5):
		pici = hspace
		for j in range(5):
			picj = np.zeros((75, 75, 3))
			picj[:,:,0] = Norm(band1[i*5+j,:,:])
			picj[:,:,1] = Norm(band2[i*5+j,:,:])
			pici = np.hstack(( pici, picj, hspace))

		picAll = np.vstack((picAll, pici, vspace))


	imgplot = plt.imshow(picAll, cmap="binary", interpolation='none') 
	plt.show()


def getCNN(imgsize):
	from keras.layers.normalization import BatchNormalization
	#from keras import regularizers

	p_activation = "relu"
	input_1 = Input(shape=(imgsize, imgsize, 3))
	#input_2 = Input(shape=[1], name="angle")
	
	convFilters = [32, 64, 128, 128, 128]
	
	c = input_1
	for i in range(len(convFilters)):
		c = Conv2D(convFilters[i], kernel_size = (3,3), activation=p_activation) (c)
		c = MaxPooling2D((2,2)) (c)
		c = Dropout(0.2)(c)

	d = Flatten()(c)
	d = BatchNormalization()(d)

	denseFilters = [256,128,128, 64]
	
	for i in range(len(denseFilters)):
		d = Dense(denseFilters[i], activation=p_activation)(d)
		d = Dropout(0.2)(d)
	
	output = Dense(1, activation="sigmoid")(d)

	model = Model(input_1,  output)

	#optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)
	optimizer = Adam(lr=5e-5) #1e-4
	#optimizer = SGD(lr=1e-1, momentum=0.9, nesterov=True)
	model.compile(loss="binary_crossentropy", optimizer=optimizer, metrics=["accuracy"])
	model.summary()
	return model


def getCNNpca(imgsize):
	input1 = Input(shape=(imgsize, imgsize, 3))
	input2 = Input(shape=(25*3,))

	x = Conv2D(64, (3, 3), activation='relu', padding='same')(input1)
	x = MaxPooling2D((2, 2), strides=(2, 2))(x)
	x = Dropout(0.2)(x)

	x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)
	x = MaxPooling2D((2, 2), strides=(2, 2))(x)
	x = Dropout(0.2)(x)

	x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)
	x = MaxPooling2D((2, 2), strides=(2, 2))(x)
	x = Dropout(0.2)(x)

	x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
	x = MaxPooling2D((2, 2), strides=(2, 2))(x)
	x = Dropout(0.2)(x)
	
	#Flatten the data for upcoming dense layers
	x = Flatten()(x)
	x = concatenate([x, input2])
	#Dense Layers
	x = Dense(2048, activation='relu')(x)
	x = Dense(512, activation='relu')(x)
	x = Dense(512, activation='relu')(x)
	x = Dense(256, activation='relu')(x)
	#Sigmoid Layer
	done = Dense(1, activation='sigmoid')(x)
	
	gmodel = Model(inputs=([input1, input2]), outputs=done)
	mypotim=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)
	gmodel.compile(loss='binary_crossentropy',
		optimizer=mypotim,
		metrics=['accuracy'])
	gmodel.summary()
	return gmodel


def getAE():
	# Encode
	inputs = Input(shape=(75, 75, 3))  # adapt this if using `channels_first` image data format

	x = Conv2D(5, (3, 3), activation='relu', padding='same')(inputs)
	#x = Dropout(0.2)(x)

	x = Conv2D(15, (3, 3), activation='relu', padding='same')(x)
	x = MaxPooling2D((3, 3), padding='same')(x)
	#x = Dropout(0.2)(x)

	x = Conv2D(75, (3, 3), activation='relu', padding='same')(x)
	x = MaxPooling2D((5, 5), padding='same')(x)
	#clayers = Dropout(0.2)(x)
	clayers = x
	x = Flatten()(clayers)
	x = Dense(125, activation='relu')(x)
	x = Dense(125, activation='relu')(x)
	encoded = Dense(75, activation='relu',activity_regularizer=regularizers.l1(10e-5))(x)

	# Decode
	x = Dense(125, activation='relu')(encoded)
	x = Dense(125, activation='relu')(x)
	x = Reshape((5,5,5))(x)
	
	#x = Dropout(0.2)(x)
	x = UpSampling2D((5, 5))(x)
	x = Conv2D(75, (3, 3), activation='relu', padding='same')(x)
	
	#x = Dropout(0.2)(x)
	x = UpSampling2D((3, 3))(x)
	x = Conv2D(15, (3, 3), activation='relu', padding='same')(x)
	
	#x = Dropout(0.2)(x)
	x = Conv2D(5, (3, 3), activation='relu', padding='same')(x)
	decoded = Conv2D(3, (2, 2), activation='linear', padding='same')(x)
	
	#print(x._keras_shape)
	# Define encoder and autoencoder models
	encoder = Model(inputs, encoded)
	AE = Model(inputs, decoded)
	'''
	# Rebuild decoder layers and define decoder
	encodedInputs = Input(shape=(128,))
	decoderL1 = AE.layers[-6](encodedInputs)
	decoderL2 = AE.layers[-5](decoderL1)
	decoderL3 = AE.layers[-4](decoderL2)
	decoderL4 = AE.layers[-3](decoderL3)
	decoderL5 = AE.layers[-2](decoderL4)
	decoderL6 = AE.layers[-1](decoderL5)
	decoder = Model(encodedInputs, decoderL6)
	'''
	optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)
	AE.compile(optimizer=optimizer, loss='mse', metrics=["accuracy"])
	return AE, encoder

def getcnn(imgsize):
	from keras import initializers

	def init():
    		return initializers.RandomUniform(minval= -0.05, maxval= 0.05, seed=None)

	#Building the model
	gmodel=Sequential()
	#Conv Layer 1
	gmodel.add(Conv2D(128,kernel_size=(3, 3),activation='relu',
		input_shape=(imgsize,imgsize,3),padding='valid',kernel_initializer=init() ))
	gmodel.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))
	gmodel.add(Dropout(0.2))
	
	#Conv Layer 2
	gmodel.add(Conv2D(128, kernel_size=(3, 3),activation='relu',padding='valid',
		kernel_initializer=init() ))
	gmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
	gmodel.add(Dropout(0.2))

	#Conv Layer 3
	gmodel.add(Conv2D(64, kernel_size=(3, 3),activation='relu',padding='valid',
		kernel_initializer=init() ))
	gmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
	gmodel.add(Dropout(0.2))
	
	#Conv Layer 4
	gmodel.add(Conv2D(64, kernel_size=(3, 3),activation='relu',padding='valid',
		kernel_initializer=init() ))
	gmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
	gmodel.add(Dropout(0.2))

	#Flatten the data for upcoming dense layers
	gmodel.add(Flatten())
	
	#Dense Layers
	gmodel.add(Dense(128,kernel_initializer=init() ))
	gmodel.add(Activation('relu'))
	gmodel.add(Dropout(0.1))

	#Dense Layer 2
	gmodel.add(Dense(64,kernel_initializer=init() ))
	gmodel.add(Activation('relu'))
	gmodel.add(Dropout(0.1))
	
	#Sigmoid Layer
	gmodel.add(Dense(1,kernel_initializer=init() ))
	gmodel.add(Activation('sigmoid'))
	#from keras.optimizers import SGD
	#mypotim=Adamax(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-05, decay=0.0) # eps = 1e-8
	#mypotim = SGD(lr=0.001, momentum=0.01, decay=0.0, nesterov=True)
	mypotim=Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0) # eps = 1e-8
	gmodel.compile(loss='binary_crossentropy',optimizer=mypotim,metrics=['accuracy'])
	gmodel.summary()
	return gmodel

def getcnn2(imgsize):
	from keras.layers.normalization import BatchNormalization
	from keras.layers import GlobalMaxPooling2D
	bn_model = 0
	p_activation = "elu"
	input_1 = Input(shape=(imgsize, imgsize, 3), name="X_1")
	#input_2 = Input(shape=[1], name="angle")

	img_1 = Conv2D(32, kernel_size = (3,3), activation=p_activation) ((BatchNormalization(momentum=bn_model))(input_1))
	img_1 = Conv2D(32, kernel_size = (3,3), activation=p_activation) (img_1)
	img_1 = MaxPooling2D((2,2)) (img_1)
	img_1 = Dropout(0.2)(img_1)
	img_1 = Conv2D(64, kernel_size = (3,3), activation=p_activation) (img_1)
	img_1 = Conv2D(64, kernel_size = (3,3), activation=p_activation) (img_1)
	img_1 = MaxPooling2D((2,2)) (img_1)
	img_1 = Dropout(0.2)(img_1)
	img_1 = Conv2D(128, kernel_size = (3,3), activation=p_activation) (img_1)
	img_1 = Conv2D(128, kernel_size = (3,3), activation=p_activation) (img_1)
	img_1 = MaxPooling2D((2,2)) (img_1)
	img_1 = Dropout(0.2)(img_1)
	img_1 = Conv2D(128, kernel_size = (3,3), activation=p_activation) (img_1)
	img_1 = MaxPooling2D((2,2)) (img_1)
	img_1 = Dropout(0.2)(img_1)
	img_1 = GlobalMaxPooling2D() (img_1)


	img_2 = Conv2D(128, kernel_size = (3,3), activation=p_activation) ((BatchNormalization(momentum=bn_model))(input_1))
	img_2 = MaxPooling2D((2,2)) (img_2)
	img_2 = Dropout(0.2)(img_2)
	img_2 = GlobalMaxPooling2D() (img_2)

	#img_concat =  (Concatenate()([img_1, img_2, BatchNormalization(momentum=bn_model)(input_2)]))

	dense_layer = BatchNormalization(momentum=bn_model) ( Dense(256, activation=p_activation)(img_2) )
	dense_layer = Dropout(0.2)(dense_layer)
	dense_layer = BatchNormalization(momentum=bn_model) ( Dense(128, activation=p_activation)(dense_layer) )
	dense_layer = Dropout(0.2)(dense_layer)
	output = Dense(1, activation="sigmoid")(dense_layer)

	model = Model(input_1,  output)
	optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)
	model.compile(loss="binary_crossentropy", optimizer=optimizer, metrics=["accuracy"])
	model.summary()
	return model

# We choose a high patience so the algorthim keeps searching even after finding a maximum
def get_callbacks(filepath, patience=8):	
	es = EarlyStopping('val_loss', patience=patience, mode="min")
	msave = ModelCheckpoint(filepath,monitor='val_loss',save_best_only=True,save_weights_only=True)
	reduce_lr = ReduceLROnPlateau(monitor='val_loss',factor=0.2,patience=7,min_lr=0.0005,mode="min")
	return [es, msave, reduce_lr]


#----------STARTS HERE----------STARTS HERE----------STARTS HERE----------STARTS HERE


'''
train = pd.read_json("data/train.json")	# 1604 x 5


filename = "data/test.json"
with open(filename, 'r') as f:
    objects = ijson.items(f, 'meta.view.columns.item')
    columns = list(objects)

print columns.shape


# Read out the data in the two bands, as 1604 x 75 x 75 arrays
TRb1, TRb2, TRname, TRlabel, TRangle, TRonlyAngle = DataSort(train)
'''

# DATA PREP
def cnn(xtr, ytr, xte, yte, unlab, h, flip, ind):
	# Use a seed based on the index
	np.random.seed(7)
	
	epo = 300
	bsize = 25
	imgsize = xtr.shape[1]
	saveStr = 'iceunsup2/Epo'+str(epo)+'Bsize'+str(bsize)+'h'+str(h)+'flip'+str(flip)+'ind'+str(ind)
	saveStr = 'weights/' + saveStr + '.hdf5' #'{epoch:02d}-{val_loss:.2f}.hdf5'

	# Denoise the images as an augmentation to the dataset. doubles dataset size
	if h != 0:
		xtr, ytr = iceDataPrep.augmentDenoise(xtr, ytr, h)

	# Trim and translate the training set and center trim the test set. quadruples dataset size
	if flip != 0:
		xtr, ytr = iceDataPrep.augmentFlip(xtr, ytr)

	# KERAS NEURAL NETWORK
	'''
	# Get or make the model. Need a different model for each trimsize
	if os.path.exists('models/iceModel' + str(imgsize)):# and ind!=0 and ind!=50: #and ind!=100:
		model = load_model('models/iceModel' + str(imgsize) )
	else:
	'''
	model = getCNN(imgsize)
	os.remove('models/iceModel' + str(imgsize) )
	model.save('models/iceModel' + str(imgsize) )
	'''
	# Get or do the run. No need to run things more than necessary, right?
	if os.path.exists(saveStr):
		print 'Pulling index', ind, 'from previous runs'
		model.load_weights(saveStr)
		scores = model.evaluate(xte, yte, verbose=0)
		if scores[1] < 0.90:
			os.remove(saveStr)
			print ' '
			print "Bad saved trial due to testing acc < 90%. Rerunning ..."
			print ' '
			callbacks = get_callbacks(filepath=saveStr, patience=80)
			# Fit the model
			model.fit(xtr, ytr,
				batch_size=bsize,
				epochs=epo,
				verbose=2,
				validation_data=(xte, yte),
				callbacks=callbacks)
	
	else:
	'''
	callbacks = get_callbacks(filepath=saveStr, patience=80)
	# Fit the model
	model.fit(xtr, ytr,
		batch_size=bsize,
		epochs=epo,
		verbose=2,
		validation_data=(xte, yte),
		callbacks=callbacks)

	# evaluate the model
	model.load_weights(saveStr)

	# Calculate the scores on the training and testing data
	results = np.zeros((2, 2))
	# Training
	scores = model.evaluate(xtr, ytr, verbose=0)
	results[0, 0] = scores[0]
	results[0, 1] = scores[1]
	# Testing
	scores = model.evaluate(xte, yte, verbose=0)
	results[1, 0] = scores[0]
	results[1, 1] = scores[1]

	prediction = model.predict(unlab)

	# If result is bad, redo run
	if results[1, 1] < 0.90:
		os.remove(saveStr)
		print ' '
		print 'Testing acc at', results[1, 1]*100.0, "%"
		print "Rerunning trial, due to testing acc < 90%"
		print ' '
		prediction, results = cnn(xtr, ytr, xte, yte, unlab, h, flip, ind)	

	return prediction.flatten(), results

def cnnPCA(xtr, pcatr, ytr, xte, pcate, yte, unlab, ind):
# Use a seed based on the index
	np.random.seed(ind)

	epo = 150
	bsize = 100
	imgsize = xtr.shape[1]
	saveStr = 'iceunsup2/cnnPCAEpo'+str(epo)+'Bsize'+str(bsize)+'ind'+str(ind)
	saveStr = 'weights/' + saveStr + '.hdf5' #'{epoch:02d}-{val_loss:.2f}.hdf5'

	# KERAS NEURAL NETWORK

	# Get or make the model. Need a different model for each trimsize
	#if os.path.exists('models/iceModel' + str(imgsize) ):
	#	model = load_model('models/iceModel' + str(imgsize) )
	#else:
	model = getCNNpca(imgsize)
	model.save('models/iceCnnPca' + str(imgsize) )
	
	# Get or do the run. No need to run things more than necessary, right?
	#if os.path.exists(saveStr):
	#	print 'Pulling index', ind, 'from previous runs'
	#	model.load_weights(saveStr)
	
	#else:
	callbacks = get_callbacks(filepath=saveStr, patience=80)
		# Fit the model
	model.fit([xtr, pcatr], ytr,
		batch_size=bsize,
		epochs=epo,
		verbose=2,
		validation_data=([xte, pcate], yte),
		callbacks=callbacks)

	# evaluate the model
	#model.load_weights(saveStr)

	# Calculate the scores on the training and testing data
	results = np.zeros((2, 2))
	# Training
	scores = model.evaluate([xtr, pcatr], ytr, verbose=0)
	results[0, 0] = scores[0]
	results[0, 1] = scores[1]
	# Testing
	scores = model.evaluate([xte, pcate], yte, verbose=0)
	results[1, 0] = scores[0]
	results[1, 1] = scores[1]

	prediction = model.predict(unlab)

	return prediction.flatten(), results









def autoencoder(xtr, ytr, xte, yte, unlab, flip):
	# Use a seed based on the index
	np.random.seed(flip)

	epo = 70
	bsize = 100
	imgsize = 75
	saveStr = 'iceunsup2/AEepo'+str(epo)+'Bsize'+str(bsize)+'flip'+str(flip)
	saveStr = 'weights/' + saveStr + '.hdf5' #'{epoch:02d}-{val_loss:.2f}.hdf5'

	# We train on both xtr and on the unlabelled images
	xtr = np.concatenate((xtr, unlab), axis=0)
	
	# Trim and translate the training set and center trim the test set. quadruples dataset size
	if flip != 0:
		xtr, ytr = iceDataPrep.augmentFlip(xtr, ytr)

	# KERAS NEURAL NETWORK
	# Get or make the model. Need a different model for each trimsize
	if os.path.exists('models/iceAE' + str(imgsize) ):
		autoencoder = load_model('models/iceAE' + str(imgsize) )
		encoder = load_model('models/iceEncoder' + str(imgsize) )
		#decoder = load_model('models/iceDecoder' + str(imgsize) )
	else:
		autoencoder, encoder = getAE()
		autoencoder.save('models/iceAE' + str(imgsize) )
		encoder.save('models/iceEncoder' + str(imgsize) )
		#decoder.save('models/iceDecoder' + str(imgsize) )
	'''	
	# Get or do the run. No need to run things more than necessary, right?
	if os.path.exists(saveStr):
		print 'Pulling index', ind, 'from previous runs'
		autoencoder.load_weights(saveStr)
	else:
		callbacks = get_callbacks(filepath=saveStr, patience=20)
		# Fit the model
		autoencoder.fit(xtr, ytr,
			batch_size=bsize,
			epochs=epo,
			verbose=2,
			validation_data=(xte, yte),
			callbacks=callbacks)
	'''
	'''
	autoencoder, encoder = getAE()
	autoencoder.save('models/iceAE' + str(imgsize) )
	encoder.save('models/iceEncoder' + str(imgsize) )
	plot_model(autoencoder, to_file = 'results/modelAEauto.png', show_shapes = True)
	plot_model(encoder, to_file = 'results/modelAEencode.png', show_shapes = True)
	'''

	callbacks = get_callbacks(filepath=saveStr, patience=20)
	# Fit the model
	autoencoder.fit(xtr, xtr,
		batch_size=bsize,
		epochs=epo,
		verbose=2,
		validation_data=(xte, xte),
		callbacks=callbacks)

	# evaluate the model
	autoencoder.load_weights(saveStr)
	# Calculate the scores on the training and testing data
	results = np.zeros((2, 2))
	# Training
	scores = autoencoder.evaluate(xtr, xtr, verbose=0)
	results[0, 0] = scores[0]
	results[0, 1] = scores[1]
	# Testing
	scores = autoencoder.evaluate(xte, xte, verbose=0)
	results[1, 0] = scores[0]
	results[1, 1] = scores[1]

	xtrPred = encoder.predict(xtr)
	unlabPred = encoder.predict(unlab)

	return xtrPred, unlabPred, results








