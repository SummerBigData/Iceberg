
import numpy as np

# Keras stuff
import keras
from keras import regularizers
from keras.models import Sequential, load_model, Model
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Activation, Input
from keras.layers import Conv2DTranspose, Reshape, UpSampling2D
from keras.optimizers import Adam
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping
from keras.utils import plot_model
import os.path # To check if a file exists
import iceDataPrep


#---------GLOBAL VARIABLES----------GLOBAL VARIABLES----------GLOBAL VARIABLES----------GLOBAL VARIABLES


# fix random seed for reproducibility
np.random.seed(7)




#----------DEFINITIONS HERE----------DEFINITIONS HERE----------DEFINITIONS HERE----------DEFINITIONS HERE



def ShowSquare(band1, band2): 
	hspace = np.zeros((75, 5, 3))
	vspace = np.zeros((5, 5*75 + 5*6, 3))
	picAll = vspace
	for i in range(5):
		pici = hspace
		for j in range(5):
			picj = np.zeros((75, 75, 3))
			picj[:,:,0] = Norm(band1[i*5+j,:,:])
			picj[:,:,1] = Norm(band2[i*5+j,:,:])
			pici = np.hstack(( pici, picj, hspace))

		picAll = np.vstack((picAll, pici, vspace))


	imgplot = plt.imshow(picAll, cmap="binary", interpolation='none') 
	plt.show()


def getCNN(imgsize):
	#Building the model
	gmodel=Sequential()
	#Conv Layer 1
	gmodel.add(Conv2D(64, kernel_size=(3, 3),activation='relu', input_shape=(imgsize, imgsize, 3)))
	gmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
	gmodel.add(Dropout(0.2))
	'''
	#Conv Layer 2
	gmodel.add(Conv2D(128, kernel_size=(3, 3), activation='relu' ))
	gmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
	gmodel.add(Dropout(0.2))

	#Conv Layer 3
	gmodel.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
	gmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
	gmodel.add(Dropout(0.2))
	
	#Conv Layer 4
	gmodel.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
	gmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
	gmodel.add(Dropout(0.2))
	'''
	#Flatten the data for upcoming dense layers
	gmodel.add(Flatten())
	
	#Dense Layers
	gmodel.add(Dense(512))
	gmodel.add(Activation('relu'))
	gmodel.add(Dropout(0.2))

	#Dense Layer 2
	gmodel.add(Dense(256))
	gmodel.add(Activation('relu'))
	gmodel.add(Dropout(0.2))
	
	#Sigmoid Layer
	gmodel.add(Dense(1))
	gmodel.add(Activation('sigmoid'))

	mypotim=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)
	gmodel.compile(loss='binary_crossentropy',
		optimizer=mypotim,
		metrics=['accuracy'])
	gmodel.summary()
	return gmodel

def getAE():
	'''
	# Encoder
	inputs = Input(shape=(75, 75, 3))

	c1 = Conv2D(32, kernel_size=(2, 2),activation='relu', padding='same')(inputs)
	p1 = MaxPooling2D((2, 2), padding='same')(c1)
	d1 = Dropout(0.2)(p1)

	c2 = Conv2D(16, kernel_size=(3, 3),activation='relu', padding='same')(d1)
	p2 = MaxPooling2D((2, 2), padding='same')(c1)
	d2 = Dropout(0.2)(c2)
	f1 = Flatten()(d2)

	D1 = Dense(75*75*16, activation='relu')(f1)
	D2 = Dense(128, activation='relu')(D1)
	D3 = Dense(64, activation='relu',activity_regularizer=regularizers.l1(10e-5))(D2)
	
	# Decoder
	D4 = Dense(128, activation='relu')(D3)
	D5 = Dense(75*75*16, activation='relu')(D4)
	re = Reshape((75, 75, 16))(D5)

	d3 = Dropout(0.2)(re)
	dc1 = Conv2DTranspose(16, kernel_size=(3, 3), activation='relu', padding='same')(d3)
	d4 = Dropout(0.2)(dc1)
	dc2 = Conv2DTranspose(3, kernel_size=(2, 2), activation='relu', padding='same')(d4)
	'''
	# Encode
	inputs = Input(shape=(75, 75, 3))  # adapt this if using `channels_first` image data format

	x = Conv2D(5, (3, 3), activation='relu', padding='same')(inputs)
	x = Dropout(0.2)(x)

	x = Conv2D(15, (3, 3), activation='relu', padding='same')(x)
	x = MaxPooling2D((3, 3), padding='same')(x)
	x = Dropout(0.2)(x)

	x = Conv2D(75, (3, 3), activation='relu', padding='same')(x)
	x = MaxPooling2D((5, 5), padding='same')(x)
	clayers = Dropout(0.2)(x)

	x = Flatten()(clayers)
	x = Dense(125, activation='relu')(x)
	x = Dense(125, activation='relu')(x)
	encoded = Dense(75, activation='relu',activity_regularizer=regularizers.l1(10e-5))(x)

	# Decode
	x = Dense(125, activation='relu')(encoded)
	x = Dense(125, activation='relu')(x)
	x = Reshape((5,5,5))(x)
	
	x = Dropout(0.2)(x)
	x = UpSampling2D((5, 5))(x)
	x = Conv2D(75, (3, 3), activation='relu', padding='same')(x)
	
	x = Dropout(0.2)(x)
	x = UpSampling2D((3, 3))(x)
	x = Conv2D(15, (3, 3), activation='relu', padding='same')(x)
	
	x = Dropout(0.2)(x)
	x = Conv2D(5, (3, 3), activation='relu', padding='same')(x)
	decoded = Conv2D(3, (2, 2), activation='linear', padding='same')(x)
	
	#print(x._keras_shape)
	# Define encoder and autoencoder models
	encoder = Model(inputs, encoded)
	AE = Model(inputs, decoded)
	'''
	# Rebuild decoder layers and define decoder
	encodedInputs = Input(shape=(128,))
	decoderL1 = AE.layers[-6](encodedInputs)
	decoderL2 = AE.layers[-5](decoderL1)
	decoderL3 = AE.layers[-4](decoderL2)
	decoderL4 = AE.layers[-3](decoderL3)
	decoderL5 = AE.layers[-2](decoderL4)
	decoderL6 = AE.layers[-1](decoderL5)
	decoder = Model(encodedInputs, decoderL6)
	'''
	optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)
	AE.compile(optimizer=optimizer, loss='mse', metrics=["accuracy"])
	return AE, encoder


# We choose a high patience so the algorthim keeps searching even after finding a maximum
def get_callbacks(filepath, patience=8):	
	es = EarlyStopping('val_loss', patience=patience, mode="min")
	msave = ModelCheckpoint(filepath, monitor='val_loss',save_best_only=True,save_weights_only=True)
	return [es, msave]


#----------STARTS HERE----------STARTS HERE----------STARTS HERE----------STARTS HERE


'''
train = pd.read_json("data/train.json")	# 1604 x 5


filename = "data/test.json"
with open(filename, 'r') as f:
    objects = ijson.items(f, 'meta.view.columns.item')
    columns = list(objects)

print columns.shape


# Read out the data in the two bands, as 1604 x 75 x 75 arrays
TRb1, TRb2, TRname, TRlabel, TRangle, TRonlyAngle = DataSort(train)
'''

# DATA PREP
def cnn(xtr, ytr, xte, yte, unlab, h, flip, ind):
	# Use a seed based on the index
	np.random.seed(ind)

	epo = 70
	bsize = 100
	imgsize = xtr.shape[1]
	saveStr = 'iceunsup2/icePCAEpo'+str(epo)+'Bsize'+str(bsize)+'h'+str(h)+'flip'+str(flip)+'ind'+str(ind)
	saveStr = 'weights/' + saveStr + '.hdf5' #'{epoch:02d}-{val_loss:.2f}.hdf5'

	# Denoise the images as an augmentation to the dataset. doubles dataset size
	if h != 0:
		xtr, ytr = iceDataPrep.augmentDenoise(xtr, ytr, h)

	# Trim and translate the training set and center trim the test set. quadruples dataset size
	if flip != 0:
		xtr, ytr = iceDataPrep.augmentFlip(xtr, ytr)

	# KERAS NEURAL NETWORK

	# Get or make the model. Need a different model for each trimsize
	#if os.path.exists('models/iceModel' + str(imgsize) ):
	#	model = load_model('models/iceModel' + str(imgsize) )
	#else:
	model = getCNN(imgsize)
	model.save('models/iceModel' + str(imgsize) )
	
	# Get or do the run. No need to run things more than necessary, right?
	if os.path.exists(saveStr):
		print 'Pulling index', ind, 'from previous runs'
		model.load_weights(saveStr)
	
	else:
		callbacks = get_callbacks(filepath=saveStr, patience=40)
		# Fit the model
		model.fit(xtr, ytr,
			batch_size=bsize,
			epochs=epo,
			verbose=2,
			validation_data=(xte, yte),
			callbacks=callbacks)

	# evaluate the model
	model.load_weights(saveStr)

	# Calculate the scores on the training and testing data
	results = np.zeros((2, 2))
	# Training
	scores = model.evaluate(xtr, ytr, verbose=0)
	results[0, 0] = scores[0]
	results[0, 1] = scores[1]
	# Testing
	scores = model.evaluate(xte, yte, verbose=0)
	results[1, 0] = scores[0]
	results[1, 1] = scores[1]

	prediction = model.predict(unlab)

	return prediction.flatten(), results


def autoencoder(xtr, ytr, xte, yte, unlab, flip):
	# Use a seed based on the index
	np.random.seed(flip)

	epo = 70
	bsize = 100
	imgsize = 75
	saveStr = 'iceunsup2/AEepo'+str(epo)+'Bsize'+str(bsize)+'flip'+str(flip)
	saveStr = 'weights/' + saveStr + '.hdf5' #'{epoch:02d}-{val_loss:.2f}.hdf5'

	# We train on both xtr and on the unlabelled images
	xtr = np.concatenate((xtr, unlab), axis=0)
	
	# Trim and translate the training set and center trim the test set. quadruples dataset size
	if flip != 0:
		xtr, ytr = iceDataPrep.augmentFlip(xtr, ytr)

	# KERAS NEURAL NETWORK
	# Get or make the model. Need a different model for each trimsize
	if os.path.exists('models/iceAE' + str(imgsize) ):
		autoencoder = load_model('models/iceAE' + str(imgsize) )
		encoder = load_model('models/iceEncoder' + str(imgsize) )
		#decoder = load_model('models/iceDecoder' + str(imgsize) )
	else:
		autoencoder, encoder = getAE()
		autoencoder.save('models/iceAE' + str(imgsize) )
		encoder.save('models/iceEncoder' + str(imgsize) )
		#decoder.save('models/iceDecoder' + str(imgsize) )
	'''	
	# Get or do the run. No need to run things more than necessary, right?
	if os.path.exists(saveStr):
		print 'Pulling index', ind, 'from previous runs'
		autoencoder.load_weights(saveStr)
	else:
		callbacks = get_callbacks(filepath=saveStr, patience=20)
		# Fit the model
		autoencoder.fit(xtr, ytr,
			batch_size=bsize,
			epochs=epo,
			verbose=2,
			validation_data=(xte, yte),
			callbacks=callbacks)
	'''
	'''
	autoencoder, encoder = getAE()
	autoencoder.save('models/iceAE' + str(imgsize) )
	encoder.save('models/iceEncoder' + str(imgsize) )
	plot_model(autoencoder, to_file = 'results/modelAEauto.png', show_shapes = True)
	plot_model(encoder, to_file = 'results/modelAEencode.png', show_shapes = True)
	'''

	callbacks = get_callbacks(filepath=saveStr, patience=20)
	# Fit the model
	autoencoder.fit(xtr, xtr,
		batch_size=bsize,
		epochs=epo,
		verbose=2,
		validation_data=(xte, xte),
		callbacks=callbacks)

	# evaluate the model
	autoencoder.load_weights(saveStr)
	# Calculate the scores on the training and testing data
	results = np.zeros((2, 2))
	# Training
	scores = autoencoder.evaluate(xtr, xtr, verbose=0)
	results[0, 0] = scores[0]
	results[0, 1] = scores[1]
	# Testing
	scores = autoencoder.evaluate(xte, xte, verbose=0)
	results[1, 0] = scores[0]
	results[1, 1] = scores[1]

	xtrPred = encoder.predict(xtr)
	unlabPred = encoder.predict(unlab)

	return xtrPred, unlabPred, results








