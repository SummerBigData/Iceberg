
import numpy as np

# Keras stuff
import keras
from keras import regularizers
from keras.models import Sequential, load_model, Model
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Activation, Input
from keras.layers import Conv2DTranspose, Reshape, UpSampling2D, concatenate
from keras.optimizers import Adam
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau
from keras.utils import plot_model
import os.path # To check if a file exists
import iceDataPrep


#---------GLOBAL VARIABLES----------GLOBAL VARIABLES----------GLOBAL VARIABLES----------GLOBAL VARIABLES


# fix random seed for reproducibility
np.random.seed(7)




#----------DEFINITIONS HERE----------DEFINITIONS HERE----------DEFINITIONS HERE----------DEFINITIONS HERE



def ShowSquare(band1, band2): 
	hspace = np.zeros((75, 5, 3))
	vspace = np.zeros((5, 5*75 + 5*6, 3))
	picAll = vspace
	for i in range(5):
		pici = hspace
		for j in range(5):
			picj = np.zeros((75, 75, 3))
			picj[:,:,0] = Norm(band1[i*5+j,:,:])
			picj[:,:,1] = Norm(band2[i*5+j,:,:])
			pici = np.hstack(( pici, picj, hspace))

		picAll = np.vstack((picAll, pici, vspace))


	imgplot = plt.imshow(picAll, cmap="binary", interpolation='none') 
	plt.show()


def getCNN(imgsize):
	#Building the model
	model=Sequential()
	
	# Conv block 1
	model.add(Conv2D(64,kernel_size=(3, 3),activation='relu',input_shape=(imgsize,imgsize,3)))
	model.add(Conv2D(64, kernel_size=(3, 3), activation='relu' ))
	model.add(Conv2D(64, kernel_size=(3, 3), activation='relu' ))
	model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))

	# Conv block 2
	model.add(Conv2D(128, kernel_size=(3, 3), activation='relu' ))
	model.add(Conv2D(128, kernel_size=(3, 3), activation='relu' ))
	model.add(Conv2D(128, kernel_size=(3, 3), activation='relu' ))
	model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))

	# Conv block 3
	model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
	model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))

	#Conv block 4
	model.add(Conv2D(256, kernel_size=(3, 3), activation='relu'))
	model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))

	# Flatten before dense
	model.add(Flatten())

	#Dense 1
	model.add(Dense(1024, activation='relu'))
	#model.add(Dropout(0.4))

	#Dense 2
	model.add(Dense(512, activation='relu'))
	#model.add(Dropout(0.2))

	# Output 
	model.add(Dense(1, activation="sigmoid"))

	mypotim=Adam(lr=0.0001, decay=0.0)
	model.compile(loss='binary_crossentropy',optimizer=mypotim,metrics=['accuracy'])
	model.summary()
	return model


def getCNNpca(imgsize):
	input1 = Input(shape=(imgsize, imgsize, 3))
	input2 = Input(shape=(25*3,))

	x = Conv2D(64, (3, 3), activation='relu', padding='same')(input1)
	x = MaxPooling2D((2, 2), strides=(2, 2))(x)
	x = Dropout(0.2)(x)

	x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)
	x = MaxPooling2D((2, 2), strides=(2, 2))(x)
	x = Dropout(0.2)(x)

	x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)
	x = MaxPooling2D((2, 2), strides=(2, 2))(x)
	x = Dropout(0.2)(x)

	x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
	x = MaxPooling2D((2, 2), strides=(2, 2))(x)
	x = Dropout(0.2)(x)
	
	#Flatten the data for upcoming dense layers
	x = Flatten()(x)
	x = concatenate([x, input2])
	#Dense Layers
	x = Dense(2048, activation='relu')(x)
	x = Dense(512, activation='relu')(x)
	x = Dense(512, activation='relu')(x)
	x = Dense(256, activation='relu')(x)
	#Sigmoid Layer
	done = Dense(1, activation='sigmoid')(x)
	
	gmodel = Model(inputs=([input1, input2]), outputs=done)
	mypotim=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)
	gmodel.compile(loss='binary_crossentropy',
		optimizer=mypotim,
		metrics=['accuracy'])
	gmodel.summary()
	return gmodel


def getAE():
	# Encode
	inputs = Input(shape=(75, 75, 3))  # adapt this if using `channels_first` image data format

	x = Conv2D(5, (3, 3), activation='relu', padding='same')(inputs)
	#x = Dropout(0.2)(x)

	x = Conv2D(15, (3, 3), activation='relu', padding='same')(x)
	x = MaxPooling2D((3, 3), padding='same')(x)
	#x = Dropout(0.2)(x)

	x = Conv2D(75, (3, 3), activation='relu', padding='same')(x)
	x = MaxPooling2D((5, 5), padding='same')(x)
	#clayers = Dropout(0.2)(x)
	clayers = x
	x = Flatten()(clayers)
	x = Dense(125, activation='relu')(x)
	x = Dense(125, activation='relu')(x)
	encoded = Dense(75, activation='relu',activity_regularizer=regularizers.l1(10e-5))(x)

	# Decode
	x = Dense(125, activation='relu')(encoded)
	x = Dense(125, activation='relu')(x)
	x = Reshape((5,5,5))(x)
	
	#x = Dropout(0.2)(x)
	x = UpSampling2D((5, 5))(x)
	x = Conv2D(75, (3, 3), activation='relu', padding='same')(x)
	
	#x = Dropout(0.2)(x)
	x = UpSampling2D((3, 3))(x)
	x = Conv2D(15, (3, 3), activation='relu', padding='same')(x)
	
	#x = Dropout(0.2)(x)
	x = Conv2D(5, (3, 3), activation='relu', padding='same')(x)
	decoded = Conv2D(3, (2, 2), activation='linear', padding='same')(x)
	
	#print(x._keras_shape)
	# Define encoder and autoencoder models
	encoder = Model(inputs, encoded)
	AE = Model(inputs, decoded)
	'''
	# Rebuild decoder layers and define decoder
	encodedInputs = Input(shape=(128,))
	decoderL1 = AE.layers[-6](encodedInputs)
	decoderL2 = AE.layers[-5](decoderL1)
	decoderL3 = AE.layers[-4](decoderL2)
	decoderL4 = AE.layers[-3](decoderL3)
	decoderL5 = AE.layers[-2](decoderL4)
	decoderL6 = AE.layers[-1](decoderL5)
	decoder = Model(encodedInputs, decoderL6)
	'''
	optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)
	AE.compile(optimizer=optimizer, loss='mse', metrics=["accuracy"])
	return AE, encoder


# We choose a high patience so the algorthim keeps searching even after finding a maximum
def get_callbacks(filepath, patience=8):	
	es = EarlyStopping('val_loss', patience=patience, mode="min")
	msave = ModelCheckpoint(filepath, monitor='val_loss',save_best_only=True,save_weights_only=True)
	reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)
	return [es, msave, reduce_lr]


#----------STARTS HERE----------STARTS HERE----------STARTS HERE----------STARTS HERE


'''
train = pd.read_json("data/train.json")	# 1604 x 5


filename = "data/test.json"
with open(filename, 'r') as f:
    objects = ijson.items(f, 'meta.view.columns.item')
    columns = list(objects)

print columns.shape


# Read out the data in the two bands, as 1604 x 75 x 75 arrays
TRb1, TRb2, TRname, TRlabel, TRangle, TRonlyAngle = DataSort(train)
'''

# DATA PREP
def cnn(xtr, ytr, xte, yte, unlab, h, flip, ind):
	# Use a seed based on the index
	np.random.seed(ind)

	epo = 70
	bsize = 100
	imgsize = xtr.shape[1]
	saveStr = 'iceunsup2/Epo'+str(epo)+'Bsize'+str(bsize)+'h'+str(h)+'flip'+str(flip)+'ind'+str(ind)
	saveStr = 'weights/' + saveStr + '.hdf5' #'{epoch:02d}-{val_loss:.2f}.hdf5'

	# Denoise the images as an augmentation to the dataset. doubles dataset size
	if h != 0:
		xtr, ytr = iceDataPrep.augmentDenoise(xtr, ytr, h)

	# Trim and translate the training set and center trim the test set. quadruples dataset size
	if flip != 0:
		xtr, ytr = iceDataPrep.augmentFlip(xtr, ytr)

	# KERAS NEURAL NETWORK

	# Get or make the model. Need a different model for each trimsize
	if os.path.exists('models/iceModel' + str(imgsize)) and ind!=0 and ind!=50): #and ind!=100:
		model = load_model('models/iceModel' + str(imgsize) )
	else:
		model = getCNN(imgsize)
		model.save('models/iceModel' + str(imgsize) )
	
	# Get or do the run. No need to run things more than necessary, right?
	#if os.path.exists(saveStr):
	#	print 'Pulling index', ind, 'from previous runs'
	#	model.load_weights(saveStr)
	
	#else:
	callbacks = get_callbacks(filepath=saveStr, patience=80)
		# Fit the model
	model.fit(xtr, ytr,
		batch_size=bsize,
		epochs=epo,
		verbose=2,
		validation_data=(xte, yte),
		callbacks=callbacks)

	# evaluate the model
	model.load_weights(saveStr)

	# Calculate the scores on the training and testing data
	results = np.zeros((2, 2))
	# Training
	scores = model.evaluate(xtr, ytr, verbose=0)
	results[0, 0] = scores[0]
	results[0, 1] = scores[1]
	# Testing
	scores = model.evaluate(xte, yte, verbose=0)
	results[1, 0] = scores[0]
	results[1, 1] = scores[1]

	prediction = model.predict(unlab)

	return prediction.flatten(), results

def cnnPCA(xtr, pcatr, ytr, xte, pcate, yte, unlab, ind):
# Use a seed based on the index
	np.random.seed(ind)

	epo = 70
	bsize = 100
	imgsize = xtr.shape[1]
	saveStr = 'iceunsup2/cnnPCAEpo'+str(epo)+'Bsize'+str(bsize)+'ind'+str(ind)
	saveStr = 'weights/' + saveStr + '.hdf5' #'{epoch:02d}-{val_loss:.2f}.hdf5'

	# KERAS NEURAL NETWORK

	# Get or make the model. Need a different model for each trimsize
	#if os.path.exists('models/iceModel' + str(imgsize) ):
	#	model = load_model('models/iceModel' + str(imgsize) )
	#else:
	model = getCNNpca(imgsize)
	model.save('models/iceCnnPca' + str(imgsize) )
	
	# Get or do the run. No need to run things more than necessary, right?
	#if os.path.exists(saveStr):
	#	print 'Pulling index', ind, 'from previous runs'
	#	model.load_weights(saveStr)
	
	#else:
	callbacks = get_callbacks(filepath=saveStr, patience=80)
		# Fit the model
	model.fit([xtr, pcatr], ytr,
		batch_size=bsize,
		epochs=epo,
		verbose=2,
		validation_data=([xte, pcate], yte),
		callbacks=callbacks)

	# evaluate the model
	#model.load_weights(saveStr)

	# Calculate the scores on the training and testing data
	results = np.zeros((2, 2))
	# Training
	scores = model.evaluate([xtr, pcatr], ytr, verbose=0)
	results[0, 0] = scores[0]
	results[0, 1] = scores[1]
	# Testing
	scores = model.evaluate([xte, pcate], yte, verbose=0)
	results[1, 0] = scores[0]
	results[1, 1] = scores[1]

	prediction = model.predict(unlab)

	return prediction.flatten(), results









def autoencoder(xtr, ytr, xte, yte, unlab, flip):
	# Use a seed based on the index
	np.random.seed(flip)

	epo = 70
	bsize = 100
	imgsize = 75
	saveStr = 'iceunsup2/AEepo'+str(epo)+'Bsize'+str(bsize)+'flip'+str(flip)
	saveStr = 'weights/' + saveStr + '.hdf5' #'{epoch:02d}-{val_loss:.2f}.hdf5'

	# We train on both xtr and on the unlabelled images
	xtr = np.concatenate((xtr, unlab), axis=0)
	
	# Trim and translate the training set and center trim the test set. quadruples dataset size
	if flip != 0:
		xtr, ytr = iceDataPrep.augmentFlip(xtr, ytr)

	# KERAS NEURAL NETWORK
	# Get or make the model. Need a different model for each trimsize
	if os.path.exists('models/iceAE' + str(imgsize) ):
		autoencoder = load_model('models/iceAE' + str(imgsize) )
		encoder = load_model('models/iceEncoder' + str(imgsize) )
		#decoder = load_model('models/iceDecoder' + str(imgsize) )
	else:
		autoencoder, encoder = getAE()
		autoencoder.save('models/iceAE' + str(imgsize) )
		encoder.save('models/iceEncoder' + str(imgsize) )
		#decoder.save('models/iceDecoder' + str(imgsize) )
	'''	
	# Get or do the run. No need to run things more than necessary, right?
	if os.path.exists(saveStr):
		print 'Pulling index', ind, 'from previous runs'
		autoencoder.load_weights(saveStr)
	else:
		callbacks = get_callbacks(filepath=saveStr, patience=20)
		# Fit the model
		autoencoder.fit(xtr, ytr,
			batch_size=bsize,
			epochs=epo,
			verbose=2,
			validation_data=(xte, yte),
			callbacks=callbacks)
	'''
	'''
	autoencoder, encoder = getAE()
	autoencoder.save('models/iceAE' + str(imgsize) )
	encoder.save('models/iceEncoder' + str(imgsize) )
	plot_model(autoencoder, to_file = 'results/modelAEauto.png', show_shapes = True)
	plot_model(encoder, to_file = 'results/modelAEencode.png', show_shapes = True)
	'''

	callbacks = get_callbacks(filepath=saveStr, patience=20)
	# Fit the model
	autoencoder.fit(xtr, xtr,
		batch_size=bsize,
		epochs=epo,
		verbose=2,
		validation_data=(xte, xte),
		callbacks=callbacks)

	# evaluate the model
	autoencoder.load_weights(saveStr)
	# Calculate the scores on the training and testing data
	results = np.zeros((2, 2))
	# Training
	scores = autoencoder.evaluate(xtr, xtr, verbose=0)
	results[0, 0] = scores[0]
	results[0, 1] = scores[1]
	# Testing
	scores = autoencoder.evaluate(xte, xte, verbose=0)
	results[1, 0] = scores[0]
	results[1, 1] = scores[1]

	xtrPred = encoder.predict(xtr)
	unlabPred = encoder.predict(unlab)

	return xtrPred, unlabPred, results








